























# 아래 코드를 복사하여 로컬 환경에서 실행해주세요
import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from collections import deque
import gym

# CartPole-v1 환경을 생성하고 초기화합니다. render_mode를 "human"으로 설정하여 화면에 표시합니다.
env = gym.make("CartPole-v1", render_mode="human")
state = env.reset()

# 상태 공간의 크기와 행동 공간의 크기를 정의합니다.
state_size = env.observation_space.shape[0]  # 상태의 차원 (카트의 위치, 속도, 막대의 각도, 각속도)
action_size = env.action_space.n  # 가능한 행동의 수 (왼쪽으로 밀기, 오른쪽으로 밀기)

# 경험 재생을 위한 버퍼 클래스를 정의합니다.
# 경험 재생(Experience Replay)은 강화학습에서 중요한 기법으로,
# 에이전트가 환경과 상호작용하며 얻은 경험(상태, 행동, 보상, 다음 상태)을 저장하고
# 나중에 무작위로 샘플링하여 학습에 사용합니다. 이를 통해:
# 1. 데이터의 시간적 연관성을 줄여 학습의 안정성을 높입니다.
# 2. 과거의 경험을 재사용하여 데이터 효율성을 증가시킵니다.
# 3. 드물게 발생하는 중요한 경험들을 여러 번 학습에 활용할 수 있습니다.
class ReplayBuffer:
    def __init__(self, max_size=50000):  # 버퍼의 최대 크기를 50000으로 설정
        self.buffer = deque(maxlen=max_size)

    def add(self, experience):
        # 새로운 경험을 버퍼에 추가합니다.
        self.buffer.append(experience)

    def sample(self, batch_size):
        # 버퍼에서 무작위로 batch_size만큼의 샘플을 추출합니다.
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[idx] for idx in indices]

    def size(self):
        # 현재 버퍼의 크기를 반환합니다.
        return len(self.buffer)

# 신경망 모델을 구축하는 함수를 정의합니다.
def build_model():
    model = Sequential([
        Dense(24, input_dim=state_size, activation='relu'),  # 입력층: 상태 크기
        Dense(24, activation='relu'),  # 은닉층
        Dense(action_size, activation='linear')  # 출력층: 각 행동에 대한 Q값
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')
    return model

# DQN 에이전트 클래스를 정의합니다.
class DQNAgent:
    def __init__(self):
        self.main_model = build_model()  # 주 신경망 모델
        self.target_model = build_model()  # 타겟 신경망 모델
        self.target_model.set_weights(self.main_model.get_weights())  # 타겟 모델의 가중치를 주 모델과 동일하게 초기화
        self.replay_buffer = ReplayBuffer()  # 경험 재생 버퍼
        self.gamma = 0.99  # 할인 계수
        self.epsilon = 1.0  # 탐험률 초기값
        self.epsilon_decay = 0.995  # 탐험률 감소 비율
        self.epsilon_min = 0.01  # 최소 탐험률
        self.batch_size = 64  # 학습 배치 크기

    def update_target_network(self):
        # 타겟 네트워크의 가중치를 주 네트워크의 가중치로 업데이트합니다.
        self.target_model.set_weights(self.main_model.get_weights())

    def select_action(self, state):
        # 입실론-그리디 정책에 따라 행동을 선택합니다.
        if np.random.rand() <= self.epsilon:
            return env.action_space.sample()  # 무작위 행동 선택
        q_values = self.main_model.predict(state)
        return np.argmax(q_values[0])  # 최대 Q값을 가진 행동 선택

    def train(self):
        # 경험 재생 버퍼에서 배치를 샘플링하여 학습을 수행합니다.
        if self.replay_buffer.size() < self.batch_size:
            return

        batch = self.replay_buffer.sample(self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        states = np.array(states).squeeze(axis=1)
        next_states = np.array(next_states).squeeze(axis=1)

        # 현재 상태에 대한 Q값 예측
        target_qs = self.main_model.predict(states)
        # 다음 상태에 대한 Q값 예측 (타겟 네트워크 사용)
        next_qs = self.target_model.predict(next_states)

        # Q-learning 업데이트 규칙을 적용하여 타겟 Q값을 계산
        for i in range(self.batch_size):
            if dones[i]:
                target_qs[i][actions[i]] = rewards[i]
            else:
                target_qs[i][actions[i]] = rewards[i] + self.gamma * np.max(next_qs[i])

        # 주 네트워크를 학습시킵니다.
        self.main_model.fit(states, target_qs, epochs=1, verbose=0, batch_size=32)

# DQN 에이전트 인스턴스를 생성합니다.
agent = DQNAgent()
episodes = 500  # 학습할 에피소드 수

# 학습 루프
for episode in range(episodes):
    state = env.reset()
    state = state[0] if isinstance(state, tuple) else state  # 상태가 튜플인 경우 첫 번째 요소만 사용
    state = np.reshape(state, [1, state_size])  # 상태를 적절한 형태로 변환
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)  # 행동 선택

        # 환경에서 한 스텝 진행
        step_result = env.step(action)
        if len(step_result) == 4:  # gym 버전에 따라 반환값이 다를 수 있음
            next_state, reward, done, info = step_result
        elif len(step_result) == 5:
            next_state, reward, done, truncated, info = step_result
            done = done or truncated  # truncated도 종료 조건으로 처리
        else:
            raise ValueError(f"Unexpected step result length: {len(step_result)}")

        next_state = next_state[0] if isinstance(next_state, tuple) else next_state
        next_state = np.reshape(next_state, [1, state_size])  # 다음 상태를 적절한 형태로 변환

        # 경험을 리플레이 버퍼에 저장
        agent.replay_buffer.add((state, action, reward, next_state, done))

        state = next_state
        total_reward += reward

        # 에이전트 학습
        agent.train()

    # 10 에피소드마다 타겟 네트워크 업데이트
    if episode % 10 == 0:
        agent.update_target_network()

    # 탐험률 감소
    if agent.epsilon > agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

    print(f"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}")

# 환경 종료
env.close()
