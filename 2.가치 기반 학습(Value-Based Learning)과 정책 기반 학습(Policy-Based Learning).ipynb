{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkc6X7uL85hhNFsRt0D6L8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 가치 기반 학습(Value-Based Learning)과 정책 기반 학습(Policy-Based Learning)\n","- **강화학습의 핵심 목표**: 환경과의 상호작용을 통해 기대되는 누적 보상(expected cumulative reward)을 최대화하는 정책(Policy)을 학습하는 것.\n","- 이는 에이전트가 어떤 상태에서 어떤 행동을 선택해야 할지를 학습하여 장기적으로 최적의 결과를 얻는 것을 의미합니다.\n","\n","강화학습은 크게 가치 기반 학습과 정책 기반 학습으로 나뉩니다.\n","\n","1. 가치 기반 학습 (Value-Based Learning)\n","  - 상태(state, state-action)의 가치 함수를 학습하고 이를 바탕으로 행동을 선택.\n","  - 대표적인 알고리즘: Q-Learning, SARSA.\n","  - Q-값(Q-Value): 특정 상태 $s$에서 행동 $a$를 취했을 때 기대되는 총 보상 $Q(s, a)$를 학습합니다.\n","  - 최적 정책은 다음과 같이 정의됩니다: $\\pi^*$(s) = $argmax_a Q(s, a)$\n","\n","2. 정책 기반 학습 (Policy-Based Learning)\n","  - 정책 함수 $\\pi^*(a|s)$를 직접 학습합니다.\n","  - 대표적인 알고리즘: Actor-Critic.\n","  - 연속적인 행동 공간에서도 적용 가능하며, 직접 정책을 최적화합니다.\n","\n","3. 가치 함수(Value Function)와 정책 함수(Policy Function)\n","- 가치 함수(Value Function): 상태나 상태-행동 쌍의 \"가치\"를 나타냅니다.\n","  - 상태 가치 함수 $V(s)$: 특정 상태 $(s$)의 가치. 지금부터 기대되는 return\n","  - 행동 가치 함수 $Q(s, a)$: 특정 상태 $(s$)에서 행동 $(a$)를 취했을 때의 가치. 지금 행동으로부터 기대되는 Return\n","- 정책 함수(Policy Function): 상태 $(s$)에서 행동 $(a$)를 선택할 확률을 나타냅니다.\n","\n","4. 탐험과 활용 문제 (Exploration vs Exploitation)\n","- 탐험(Exploration): 새로운 정보를 탐색하기 위해 무작위 행동을 선택.\n","- 활용(Exploitation): 현재 학습된 최적 행동을 선택하여 보상을 극대화.\n","- 균형 유지가 중요하며, $epsilon$-탐욕적 정책($epsilon$-greedy)을 사용해 해결합니다.\n","- $epsilon$-탐욕적 정책: 초기에는 탐험을 많이하고 점차 줄여나가는 정책"],"metadata":{"id":"GcAaI9xGbZWS"}},{"cell_type":"markdown","source":["# 정책 이터레이션\n","- **정책 이터레이션(Policy Iteration)**은 **가치 기반 학습(Value-Based Learning)**에 속함\n","- 정책 이터레이션은 주어진 정책 하에서 벨만 기대 방정식을 사용해 가치를 평가하고 개선\n","- 정책 이터레이션은 초기 정책으로 시작해서 점진적으로 개선하여 최적 정책에 도달하는 알고리즘\n","- 정책 이터레이션은 정책 평가와 정책 개선 단계를 번갈아 수행하면서 최적의 정책을 찾음\n","\n","- 정책 평가(Policy Evaluation):\n","  - 현재 정책 하에 각 상태의 가치를 계산. 이 과정에서, 벨만 기대 방정식을 사용하여 모든 상태에 대해 가치 함수를 반복적으로 계산하며, 이는 현재 정책을 따랐을 때 각 상태에서 기대할 수 있는 장기적인 보상의 총합을 의미함\n","\n","- 정책 개선(Policy Improvement):\n","  - 계산된 가치 함수 $V^\\pi(s)$)를 바탕으로 정책을 업데이트.\n","정책이 더 이상 개선되지 않을 때(수렴할 때)까지 두 단계를 반복\n","상태 가치 함수를 사용하여 최적의 정책을 도출\n","\n"," <img src=\"https://raw.githubusercontent.com/zoomKoding/zoomKoding.github.io/source/assets/_posts/RL2-12.png\" width=\"500\">\n","\n","\n","- Rule : 세모(-1)를 피해 동그라미(1)에 최단 경로로 도달하여 보상을 획득하는 것이 목적임\n","- 정책 평가(Evaluate) : 현재 정책에 따라 각 상태의 가치를 계산. 즉, 상태에서 가능한 모든 행동에 대한 기대 보상을 계산하고, 그 값을 바탕으로 상태의 가치를 업데이트\n","- 정책 개선(Improve) : 각 상태에서 가능한 행동 중, 가장 큰 보상(즉, 더 높은 상태 가치를 제공하는 행동)을 취할 확률이 더 높도록 정책 개선\n","Move : 현재까지의 정책을 기반으로 에이전트 이동 시작\n","\n","### 알고리즘\n","1. **초기화**:\n","   - 초기 정책 $(\\pi_0$)를 무작위로 설정.\n","2. **반복**:\n","   - **정책 평가**: 현재 정책 $\\pi_k$에 대한 가치 함수 $(V^{\\pi_k}(s)$) 계산.\n","     \n","     $$\n","     ^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s, a) \\big[ R(s, a, s') + \\gamma V^\\pi(s') \\big]\n","     $$\n","  - **정책 개선**: 가치 함수 $(V^{\\pi_k}(s)$)를 사용해 새로운 정책 $(\\pi_{k+1}$) 도출.\n","\n","     $$\n","     \\pi_{k+1}(s) = \\arg\\max_a \\sum_{s'} P(s'|s, a) \\big[ R(s, a, s') + \\gamma V^{\\pi_k}(s') \\big]\n","     $$\n","3. 정책이 수렴할 때 종료.\n","\n","### 특징\n","- **장점**: 정책과 가치 함수를 동시에 업데이트하기 때문에 수렴 속도가 빠름.\n","- **단점**: 정책 평가 단계가 반복적으로 수행되므로 계산 비용이 클 수 있음."],"metadata":{"id":"tMki0-2HmfOB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMzSwCwSbXbh"},"outputs":[],"source":["# 코드 출처 : https://github.com/rlcode/reinforcement-learning-kr\n","# 정책 이터레이션 : 각각의 파일을 py 파일로 만들어 로컬에서 실행하세요!\n","\n","# environment.py\n","# Grid World 환경 시뮬레이션 및 GUI를 제공하는 코드입니다.\n","\n","import tkinter as tk  # GUI 생성을 위한 라이브러리\n","from tkinter import Button  # 버튼 위젯 사용\n","import time  # 시간 지연을 위한 라이브러리\n","import numpy as np  # 배열 및 수학 계산을 위한 라이브러리\n","from PIL import ImageTk, Image  # 이미지를 Tkinter에 사용하기 위한 라이브러리\n","\n","PhotoImage = ImageTk.PhotoImage\n","UNIT = 100  # 그리드의 각 셀 크기 (100 x 100 픽셀)\n","HEIGHT = 5  # 그리드월드의 세로 길이 (셀 단위)\n","WIDTH = 5  # 그리드월드의 가로 길이 (셀 단위)\n","TRANSITION_PROB = 1  # 상태 전이 확률\n","POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 가능한 행동: 상, 하, 좌, 우\n","ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 행동을 좌표로 표현 (상, 하, 좌, 우)\n","REWARDS = []  # 보상 설정을 위한 리스트\n","\n","\n","class GraphicDisplay(tk.Tk):\n","    \"\"\"그리드월드 시뮬레이션을 GUI로 시각화하는 클래스\"\"\"\n","    def __init__(self, agent):\n","        super(GraphicDisplay, self).__init__()\n","        self.title('Policy Iteration')  # 창 제목 설정\n","        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT + 50))  # 창 크기 설정\n","        self.texts = []  # 캔버스 텍스트 저장\n","        self.arrows = []  # 캔버스 화살표 저장\n","        self.env = Env()  # 환경 객체 생성\n","        self.agent = agent  # 에이전트 객체 (정책 이터레이션 수행)\n","        self.evaluation_count = 0  # 평가 횟수\n","        self.improvement_count = 0  # 개선 횟수\n","        self.is_moving = 0  # 이동 여부 플래그\n","        # 이미지 로드\n","        (self.up, self.down, self.left, self.right), self.shapes = self.load_images()\n","        # 캔버스 생성\n","        self.canvas = self._build_canvas()\n","        # 보상 텍스트 추가\n","        self.text_reward(2, 2, \"R : 1.0\")\n","        self.text_reward(1, 2, \"R : -1.0\")\n","        self.text_reward(2, 1, \"R : -1.0\")\n","\n","    def _build_canvas(self):\n","        \"\"\"GUI 캔버스 및 버튼을 생성하는 메서드\"\"\"\n","        canvas = tk.Canvas(self, bg='white',\n","                           height=HEIGHT * UNIT,\n","                           width=WIDTH * UNIT)\n","        # \"Evaluate\" 버튼 추가\n","        iteration_button = Button(self, text=\"Evaluate\",\n","                                  command=self.evaluate_policy)\n","        iteration_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.13, HEIGHT * UNIT + 10,\n","                             window=iteration_button)\n","        # \"Improve\" 버튼 추가\n","        policy_button = Button(self, text=\"Improve\",\n","                               command=self.improve_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.37, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","        # \"Move\" 버튼 추가\n","        policy_button = Button(self, text=\"move\", command=self.move_by_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.62, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","        # \"Reset\" 버튼 추가\n","        policy_button = Button(self, text=\"reset\", command=self.reset)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.87, HEIGHT * UNIT + 10,\n","                             window=policy_button)\n","\n","        # 그리드 라인 생성\n","        for col in range(0, WIDTH * UNIT, UNIT):  # 세로선\n","            x0, y0, x1, y1 = col, 0, col, HEIGHT * UNIT\n","            canvas.create_line(x0, y0, x1, y1)\n","        for row in range(0, HEIGHT * UNIT, UNIT):  # 가로선\n","            x0, y0, x1, y1 = 0, row, HEIGHT * UNIT, row\n","            canvas.create_line(x0, y0, x1, y1)\n","\n","        # 캔버스에 초기 이미지 추가\n","        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])  # 이동 객체\n","        canvas.create_image(250, 150, image=self.shapes[1])  # 장애물\n","        canvas.create_image(150, 250, image=self.shapes[1])  # 장애물\n","        canvas.create_image(250, 250, image=self.shapes[2])  # 목표 지점\n","\n","        canvas.pack()\n","\n","        return canvas\n","\n","    def load_images(self):\n","        \"\"\"이미지 파일을 로드하고 크기를 조정하여 반환\"\"\"\n","        up = PhotoImage(Image.open(\"../img/up.png\").resize((13, 13)))\n","        right = PhotoImage(Image.open(\"../img/right.png\").resize((13, 13)))\n","        left = PhotoImage(Image.open(\"../img/left.png\").resize((13, 13)))\n","        down = PhotoImage(Image.open(\"../img/down.png\").resize((13, 13)))\n","        rectangle = PhotoImage(Image.open(\"../img/rectangle.png\").resize((65, 65)))\n","        triangle = PhotoImage(Image.open(\"../img/triangle.png\").resize((65, 65)))\n","        circle = PhotoImage(Image.open(\"../img/circle.png\").resize((65, 65)))\n","        return (up, down, left, right), (rectangle, triangle, circle)\n","\n","    def reset(self):\n","        \"\"\"환경을 초기 상태로 리셋\"\"\"\n","        if self.is_moving == 0:\n","            self.evaluation_count = 0\n","            self.improvement_count = 0\n","            # 기존 텍스트 및 화살표 삭제\n","            for i in self.texts:\n","                self.canvas.delete(i)\n","            for i in self.arrows:\n","                self.canvas.delete(i)\n","            # 에이전트 초기화\n","            self.agent.value_table = [[0.0] * WIDTH for _ in range(HEIGHT)]\n","            self.agent.policy_table = ([[[0.25, 0.25, 0.25, 0.25]] * WIDTH\n","                                        for _ in range(HEIGHT)])\n","            self.agent.policy_table[2][2] = []  # 목표 상태\n","            # 이동 객체 초기 위치로 이동\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","    def text_value(self, row, col, contents, font='Helvetica', size=10,\n","                   style='normal', anchor=\"nw\"):\n","        \"\"\"지정된 셀에 값을 표시하는 텍스트 추가\"\"\"\n","        origin_x, origin_y = 85, 70\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def text_reward(self, row, col, contents, font='Helvetica', size=10,\n","                    style='normal', anchor=\"nw\"):\n","        \"\"\"지정된 셀에 보상을 표시하는 텍스트 추가\"\"\"\n","        origin_x, origin_y = 5, 5\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def rectangle_move(self, action):\n","        \"\"\"현재 행동에 따라 이동 객체를 움직임\"\"\"\n","        base_action = np.array([0, 0])\n","        location = self.find_rectangle()  # 현재 위치 확인\n","        self.render()\n","        if action == 0 and location[0] > 0:  # 상\n","            base_action[1] -= UNIT\n","        elif action == 1 and location[0] < HEIGHT - 1:  # 하\n","            base_action[1] += UNIT\n","        elif action == 2 and location[1] > 0:  # 좌\n","            base_action[0] -= UNIT\n","        elif action == 3 and location[1] < WIDTH - 1:  # 우\n","            base_action[0] += UNIT\n","        # 이동\n","        self.canvas.move(self.rectangle, base_action[0], base_action[1])\n","\n","    def find_rectangle(self):\n","        \"\"\"이동 객체의 현재 위치를 반환\"\"\"\n","        temp = self.canvas.coords(self.rectangle)\n","        x = (temp[0] / 100) - 0.5\n","        y = (temp[1] / 100) - 0.5\n","        return int(y), int(x)\n","\n","    def move_by_policy(self):\n","        \"\"\"정책에 따라 이동 객체를 이동\"\"\"\n","        if self.improvement_count != 0 and self.is_moving != 1:\n","            self.is_moving = 1\n","\n","            # 초기 위치로 이동\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","            # 정책에 따라 이동\n","            x, y = self.find_rectangle()\n","            while len(self.agent.policy_table[x][y]) != 0:\n","                self.after(100,\n","                           self.rectangle_move(self.agent.get_action([x, y])))\n","                x, y = self.find_rectangle()\n","            self.is_moving = 0\n","\n","    def draw_one_arrow(self, col, row, policy):\n","        \"\"\"정책에 따라 해당 셀에 화살표를 그림\"\"\"\n","        if col == 2 and row == 2:  # 목표 지점에서는 표시 안 함\n","            return\n","\n","        if policy[0] > 0:  # 상\n","            origin_x, origin_y = 50 + (UNIT * row), 10 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.up))\n","        if policy[1] > 0:  # 하\n","            origin_x, origin_y = 50 + (UNIT * row), 90 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.down))\n","        if policy[2] > 0:  # 좌\n","            origin_x, origin_y = 10 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.left))\n","        if policy[3] > 0:  # 우\n","            origin_x, origin_y = 90 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.right))\n","\n","    def draw_from_policy(self, policy_table):\n","        \"\"\"정책 테이블에 따라 모든 셀에 화살표를 그림\"\"\"\n","        for i in range(HEIGHT):\n","            for j in range(WIDTH):\n","                self.draw_one_arrow(i, j, policy_table[i][j])\n","\n","    def print_value_table(self, value_table):\n","        \"\"\"값 함수 테이블을 화면에 표시\"\"\"\n","        for i in range(WIDTH):\n","            for j in range(HEIGHT):\n","                self.text_value(i, j, value_table[i][j])\n","\n","    def render(self):\n","        \"\"\"캔버스를 다시 렌더링\"\"\"\n","        time.sleep(0.1)\n","        self.canvas.tag_raise(self.rectangle)\n","        self.update()\n","\n","    def evaluate_policy(self):\n","        \"\"\"정책 평가 버튼 클릭 시 실행\"\"\"\n","        self.evaluation_count += 1\n","        # 기존 텍스트 삭제\n","        for i in self.texts:\n","            self.canvas.delete(i)\n","        self.agent.policy_evaluation()  # 정책 평가 수행\n","        self.print_value_table(self.agent.value_table)\n","\n","    def improve_policy(self):\n","        \"\"\"정책 개선 버튼 클릭 시 실행\"\"\"\n","        self.improvement_count += 1\n","        # 기존 화살표 삭제\n","        for i in self.arrows:\n","            self.canvas.delete(i)\n","        self.agent.policy_improvement()  # 정책 개선 수행\n","        self.draw_from_policy(self.agent.policy_table)\n","\n","\n","class Env:\n","    def __init__(self):\n","        self.transition_probability = TRANSITION_PROB\n","        self.width = WIDTH\n","        self.height = HEIGHT\n","        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n","        self.possible_actions = POSSIBLE_ACTIONS\n","        self.reward[2][2] = 1  # (2,2) 좌표 동그라미 위치에 보상 1\n","        self.reward[1][2] = -1  # (1,2) 좌표 세모 위치에 보상 -1\n","        self.reward[2][1] = -1  # (2,1) 좌표 세모 위치에 보상 -1\n","        self.all_state = []\n","\n","        for x in range(WIDTH):\n","            for y in range(HEIGHT):\n","                state = [x, y]\n","                self.all_state.append(state)\n","\n","    def get_reward(self, state, action):\n","        next_state = self.state_after_action(state, action)\n","        return self.reward[next_state[0]][next_state[1]]\n","\n","    def state_after_action(self, state, action_index):\n","        action = ACTIONS[action_index]\n","        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n","\n","    @staticmethod\n","    def check_boundary(state):\n","        state[0] = (0 if state[0] < 0 else WIDTH - 1\n","                    if state[0] > WIDTH - 1 else state[0])\n","        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n","                    if state[1] > HEIGHT - 1 else state[1])\n","        return state\n","\n","    def get_transition_prob(self, state, action):\n","        return self.transition_probability\n","\n","    def get_all_states(self):\n","        return self.all_state"]},{"cell_type":"code","source":["# policy_iteration\n","import random\n","from environment import GraphicDisplay, Env\n","\n","class PolicyIteration:\n","    def __init__(self, env):\n","        # 환경 객체를 받아서 초기화합니다.\n","        self.env = env\n","        # 각 상태에 대한 가치 함수를 2차원 리스트로 초기화합니다. 모든 값을 0으로 설정합니다.\n","        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","        # 모든 가능한 행동에 대해 동일한 확률을 가지는 정책 테이블을 초기화합니다.\n","        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]\n","        # 특정 상태 (여기서는 [2, 2])는 종료 상태로 설정합니다. 해당 상태의 정책은 비어 있습니다.\n","        self.policy_table[2][2] = []\n","        # 감가율을 설정합니다. 이는 미래 보상의 현재 가치를 결정합니다.\n","        self.discount_factor = 0.9\n","\n","    def policy_evaluation(self):\n","        # 다음 가치 함수를 저장할 임시 테이블을 초기화합니다.\n","        next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]\n","        # 모든 상태에 대해 가치 함수를 업데이트합니다.\n","        for state in self.env.get_all_states():\n","            value = 0.0\n","            # 종료 상태의 가치는 0입니다.\n","            if state == [2, 2]:\n","                next_value_table[state[0]][state[1]] = value\n","                continue\n","\n","            # 현재 정책에 따른 가치 함수 업데이트 (벨만 기대 방정식 활용)\n","            for action in self.env.possible_actions:\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n","                if state == [1, 0]:\n","                    print((self.get_policy(state)[action],(reward , self.discount_factor , next_value)))\n","                    print(next_state)\n","            next_value_table[state[0]][state[1]] = round(value, 4)\n","\n","        # 업데이트된 가치 테이블로 현재 가치 테이블을 교체합니다.\n","        self.value_table = next_value_table\n","\n","    def policy_improvement(self):\n","        # 정책을 개선하기 위한 임시 정책 테이블을 생성합니다.\n","        next_policy = self.policy_table\n","        for state in self.env.get_all_states():\n","            if state == [2, 2]:\n","                continue\n","            value = -99999\n","            max_index = []\n","            result = [0.0, 0.0, 0.0, 0.0]  # 각 행동에 대한 최적 정책 확률을 저장합니다.\n","\n","            # 각 행동에 대해 '보상 + 할인된 가치 함수'를 계산하여 최적의 행동을 찾습니다.\n","            for index, action in enumerate(self.env.possible_actions):\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                temp = reward + self.discount_factor * next_value\n","\n","                # 최대 보상을 제공하는 행동(들)을 찾습니다.\n","                if temp == value:\n","                    max_index.append(index)\n","                elif temp > value:\n","                    value = temp\n","                    max_index.clear()\n","                    max_index.append(index)\n","\n","            # 최적의 행동들에 대해 동일한 확률을 할당합니다.\n","            prob = 1 / len(max_index)\n","            for index in max_index:\n","                result[index] = prob\n","\n","            next_policy[state[0]][state[1]] = result\n","\n","        # 업데이트된 정책 테이블로 현재 정책 테이블을 교체합니다.\n","        self.policy_table = next_policy\n","        print(next_policy)\n","\n","    def get_action(self, state):\n","        # 현재 정책에 따라 무작위로 행동을 선택합니다.\n","        random_pick = random.randrange(100) / 100\n","        policy = self.get_policy(state)\n","        policy_sum = 0.0\n","        for index, value in enumerate(policy):\n","            policy_sum += value\n","            if random_pick < policy_sum:\n","                return index\n","\n","    def get_policy(self, state):\n","        # 특정 상태의 정책을 반환합니다. 종료 상태라면 0을 반환합니다.\n","        if state == [2, 2]:\n","            return 0.0\n","        return self.policy_table[state[0]][state[1]]\n","\n","    def get_value(self, state):\n","        # 가치 테이블에서 특정 상태의 가치를 반환합니다.\n","        return round(self.value_table[state[0]][state[1]], 2)\n","\n","if __name__ == \"__main__\":\n","    env = Env()\n","    policy_iteration = PolicyIteration(env)\n","    grid_world = GraphicDisplay(policy_iteration)\n","    grid_world.mainloop()"],"metadata":{"id":"h6Q5u23ArJ-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 가치 이터레이션\n","- 가치 이터레이션(Value Iteration)은 가치 기반 학습(Value-Based Learning)에 속함\n","- 가치 이터레이션은 각 반복에서 직접적으로 최적의 가치 함수를 추정하고, 이를 바탕으로 최적의 정책을 결정함.\n","- 이 방법은 정책 평가와 정책 개선 단계를 하나로 합쳐 더 효율적으로 계산할 수 있게 해줌.\n","- 가치 함수 업데이트:\n","  - 모든 상태에 대해 가능한 모든 행동의 결과를 고려하여, 각 상태의 가치 함수를 업데이트함\n","  - 벨만 최적 방정식을 사용하여, 각 상태에서 가능한 행동 중에서 최대의 기대 가치를 제공하는 행동을 통해 가치 함수를 업데이트함.\n","  - 이렇게 계산된 가치 함수를 기반으로, 각 상태에서 가능한 행동들 중에서 가장 - 높은 가치를 제공하는 행동을 선택하여 최적의 정책을 결정함.\n","- 가치 이터레이션은 일반적으로 정책 이터레이션보다 더 빠르게 수렴할 수 있으며, 계산 과정이 좀 더 단순함\n","\n"," <img src=\"https://raw.githubusercontent.com/zoomKoding/zoomKoding.github.io/source/assets/_posts/RL2-14.png\" width=\"700\">\n","\n","\n","### 알고리즘\n","1. **초기화**:\n","   - $V(s)$를 임의의 값으로 초기화 (예: $V(s) = 0$).\n","2. **반복**:\n","   - 벨만 최적 방정식을 사용해 \\(V(s)\\)를 업데이트:\n","\n","     $$\n","     V(s) \\leftarrow \\max_a \\sum_{s'} P(s'|s, a) \\big[ R(s, a, s') + \\gamma V(s') \\big]\n","     $$\n","\n","   - $V(s)$가 수렴할 때까지 반복.\n","\n","3. 최적 가치 함수 $V^*(s)$를 바탕으로 최적 정책 계산:\n","\n","$$\n","     \\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s, a) \\big[ R(s, a, s') + \\gamma V^*(s') \\big]\n","$$\n","\n","### 특징\n","- **장점**: 정책 평가와 정책 개선 단계를 한 번에 수행하므로 계산이 간단하고 직관적.\n","- **단점**: 더 많은 반복이 필요할 수 있음.\n","\n","\n","value_iteration.py 파일 실행\n","- Rule : 세모(-1)를 피해 동그라미(1)에 최단 경로로 도달하여 보상을 획득하는 것이 목적임\n","- 계산(Calculate) : 각 상태에 대해 최적의 행동을 결정하기 위한 가치를 계산하고 업데이트\n","- Print Policy : 현재 추정된 최적의 정책을 시각적으로 표시\n","- Move : 현재까지의 정책을 기반으로 에이전트 이동 시작\n","- Clear : 최초 실행 상태로 초기화"],"metadata":{"id":"IssZDpr1pAal"}},{"cell_type":"code","source":["# 코드 출처 : https://github.com/rlcode/reinforcement-learning-kr\n","# 가치 이터레이션 : 각각의 파일을 py 파일로 만들어 로컬에서 실행하세요!\n","\n","# environment.py\n","import tkinter as tk\n","import time\n","import numpy as np\n","import random\n","from PIL import ImageTk, Image\n","\n","PhotoImage = ImageTk.PhotoImage\n","UNIT = 100  # 픽셀 수\n","HEIGHT = 5  # 그리드월드 세로\n","WIDTH = 5  # 그리드월드 가로\n","TRANSITION_PROB = 1\n","POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 상, 하, 좌, 우\n","ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 좌표로 나타낸 행동\n","REWARDS = []\n","\n","\n","class GraphicDisplay(tk.Tk):\n","    def __init__(self, value_iteration):\n","        super(GraphicDisplay, self).__init__()\n","        self.title('Value Iteration')\n","        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT + 50))\n","        self.texts = []\n","        self.arrows = []\n","        self.env = Env()\n","        self.agent = value_iteration\n","        self.iteration_count = 0\n","        self.improvement_count = 0\n","        self.is_moving = 0\n","        (self.up, self.down, self.left,\n","         self.right), self.shapes = self.load_images()\n","        self.canvas = self._build_canvas()\n","        self.text_reward(2, 2, \"R : 1.0\")\n","        self.text_reward(1, 2, \"R : -1.0\")\n","        self.text_reward(2, 1, \"R : -1.0\")\n","\n","    def _build_canvas(self):\n","        canvas = tk.Canvas(self, bg='white',\n","                           height=HEIGHT * UNIT,\n","                           width=WIDTH * UNIT)\n","        # 버튼 초기화\n","        iteration_button = tk.Button(self, text=\"Calculate\",\n","                                     command=self.calculate_value)\n","        iteration_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.13, (HEIGHT * UNIT) + 10,\n","                             window=iteration_button)\n","\n","        policy_button = tk.Button(self, text=\"Print Policy\",\n","                                  command=self.print_optimal_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.37, (HEIGHT * UNIT) + 10,\n","                             window=policy_button)\n","\n","        policy_button = tk.Button(self, text=\"Move\",\n","                                  command=self.move_by_policy)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.62, (HEIGHT * UNIT) + 10,\n","                             window=policy_button)\n","\n","        policy_button = tk.Button(self, text=\"Clear\", command=self.clear)\n","        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n","        canvas.create_window(WIDTH * UNIT * 0.87, (HEIGHT * UNIT) + 10,\n","                             window=policy_button)\n","\n","        # 그리드 생성\n","        for col in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = col, 0, col, HEIGHT * UNIT\n","            canvas.create_line(x0, y0, x1, y1)\n","        for row in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = 0, row, HEIGHT * UNIT, row\n","            canvas.create_line(x0, y0, x1, y1)\n","\n","        # 캔버스에 이미지 추가\n","        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])\n","        canvas.create_image(250, 150, image=self.shapes[1])\n","        canvas.create_image(150, 250, image=self.shapes[1])\n","        canvas.create_image(250, 250, image=self.shapes[2])\n","\n","        canvas.pack()\n","\n","        return canvas\n","\n","    def load_images(self):\n","        PhotoImage = ImageTk.PhotoImage\n","        up = PhotoImage(Image.open(\"../img/up.png\").resize((13, 13)))\n","        right = PhotoImage(Image.open(\"../img/right.png\").resize((13, 13)))\n","        left = PhotoImage(Image.open(\"../img/left.png\").resize((13, 13)))\n","        down = PhotoImage(Image.open(\"../img/down.png\").resize((13, 13)))\n","        rectangle = PhotoImage(\n","            Image.open(\"../img/rectangle.png\").resize((65, 65)))\n","        triangle = PhotoImage(\n","            Image.open(\"../img/triangle.png\").resize((65, 65)))\n","        circle = PhotoImage(Image.open(\"../img/circle.png\").resize((65, 65)))\n","        return (up, down, left, right), (rectangle, triangle, circle)\n","\n","    def clear(self):\n","\n","        if self.is_moving == 0:\n","            self.iteration_count = 0\n","            self.improvement_count = 0\n","            for i in self.texts:\n","                self.canvas.delete(i)\n","\n","            for i in self.arrows:\n","                self.canvas.delete(i)\n","\n","            self.agent.value_table = [[0.0] * WIDTH for _ in range(HEIGHT)]\n","\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","    def reset(self):\n","        self.update()\n","        time.sleep(0.5)\n","        self.canvas.delete(self.rectangle)\n","        return self.canvas.coords(self.rectangle)\n","\n","    def text_value(self, row, col, contents, font='Helvetica', size=12,\n","                   style='normal', anchor=\"nw\"):\n","        origin_x, origin_y = 85, 70\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def text_reward(self, row, col, contents, font='Helvetica', size=12,\n","                    style='normal', anchor=\"nw\"):\n","        origin_x, origin_y = 5, 5\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def rectangle_move(self, action):\n","        base_action = np.array([0, 0])\n","        location = self.find_rectangle()\n","        self.render()\n","        if action == 0 and location[0] > 0:  # up\n","            base_action[1] -= UNIT\n","        elif action == 1 and location[0] < HEIGHT - 1:  # down\n","            base_action[1] += UNIT\n","        elif action == 2 and location[1] > 0:  # left\n","            base_action[0] -= UNIT\n","        elif action == 3 and location[1] < WIDTH - 1:  # right\n","            base_action[0] += UNIT\n","\n","        self.canvas.move(self.rectangle, base_action[0],\n","                         base_action[1])  # move agent\n","\n","    def find_rectangle(self):\n","        temp = self.canvas.coords(self.rectangle)\n","        x = (temp[0] / 100) - 0.5\n","        y = (temp[1] / 100) - 0.5\n","        return int(y), int(x)\n","\n","    def move_by_policy(self):\n","\n","        if self.improvement_count != 0 and self.is_moving != 1:\n","            self.is_moving = 1\n","            x, y = self.canvas.coords(self.rectangle)\n","            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","\n","            x, y = self.find_rectangle()\n","            while len(self.agent.get_action([x, y])) != 0:\n","                action = random.sample(self.agent.get_action([x, y]), 1)[0]\n","                self.after(100, self.rectangle_move(action))\n","                x, y = self.find_rectangle()\n","            self.is_moving = 0\n","\n","    def draw_one_arrow(self, col, row, action):\n","        if col == 2 and row == 2:\n","            return\n","        if action == 0:  # up\n","            origin_x, origin_y = 50 + (UNIT * row), 10 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.up))\n","        elif action == 1:  # down\n","            origin_x, origin_y = 50 + (UNIT * row), 90 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.down))\n","        elif action == 3:  # right\n","            origin_x, origin_y = 90 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.right))\n","        elif action == 2:  # left\n","            origin_x, origin_y = 10 + (UNIT * row), 50 + (UNIT * col)\n","            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n","                                                        image=self.left))\n","\n","    def draw_from_values(self, state, action_list):\n","        i = state[0]\n","        j = state[1]\n","        for action in action_list:\n","            self.draw_one_arrow(i, j, action)\n","\n","    def print_values(self, values):\n","        for i in range(WIDTH):\n","            for j in range(HEIGHT):\n","                self.text_value(i, j, values[i][j])\n","\n","    def render(self):\n","        time.sleep(0.1)\n","        self.canvas.tag_raise(self.rectangle)\n","        self.update()\n","\n","    def calculate_value(self):\n","        self.iteration_count += 1\n","        for i in self.texts:\n","            self.canvas.delete(i)\n","        self.agent.value_iteration()\n","        self.print_values(self.agent.value_table)\n","\n","    def print_optimal_policy(self):\n","        self.improvement_count += 1\n","        for i in self.arrows:\n","            self.canvas.delete(i)\n","        for state in self.env.get_all_states():\n","            action = self.agent.get_action(state)\n","            self.draw_from_values(state, action)\n","\n","\n","class Env:\n","    def __init__(self):\n","        self.transition_probability = TRANSITION_PROB\n","        self.width = WIDTH  # Width of Grid World\n","        self.height = HEIGHT  # Height of GridWorld\n","        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n","        self.possible_actions = POSSIBLE_ACTIONS\n","        self.reward[2][2] = 1  # reward 1 for circle\n","        self.reward[1][2] = -1  # reward -1 for triangle\n","        self.reward[2][1] = -1  # reward -1 for triangle\n","        self.all_state = []\n","\n","        for x in range(WIDTH):\n","            for y in range(HEIGHT):\n","                state = [x, y]\n","                self.all_state.append(state)\n","\n","    def get_reward(self, state, action):\n","        next_state = self.state_after_action(state, action)\n","        return self.reward[next_state[0]][next_state[1]]\n","\n","    def state_after_action(self, state, action_index):\n","        action = ACTIONS[action_index]\n","        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n","\n","    @staticmethod\n","    def check_boundary(state):\n","        state[0] = (0 if state[0] < 0 else WIDTH - 1\n","        if state[0] > WIDTH - 1 else state[0])\n","        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n","        if state[1] > HEIGHT - 1 else state[1])\n","        return state\n","\n","    def get_transition_prob(self, state, action):\n","        return self.transition_probability\n","\n","    def get_all_states(self):\n","        return self.all_state"],"metadata":{"id":"XPwqCXw2qjMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# value_iteration.py\n","# -*- coding: utf-8 -*-\n","from environment import GraphicDisplay, Env\n","\n","class ValueIteration:\n","    def __init__(self, env):\n","        # 환경 객체 생성\n","        self.env = env\n","        # 가치 함수를 2차원 리스트로 초기화\n","        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n","        # 감가율\n","        self.discount_factor = 0.9\n","\n","    # 가치 이터레이션\n","    # 벨만 최적 방정식을 통해 다음 가치 함수 계산\n","    def value_iteration(self):\n","        next_value_table = [[0.0] * self.env.width for _ in\n","                            range(self.env.height)]\n","        for state in self.env.get_all_states():\n","            if state == [2, 2]:\n","                next_value_table[state[0]][state[1]] = 0.0\n","                continue\n","            # 가치 함수를 위한 빈 리스트\n","            value_list = []\n","\n","            # 가능한 모든 행동에 대해 계산\n","            for action in self.env.possible_actions:\n","                next_state = self.env.state_after_action(state, action)\n","                reward = self.env.get_reward(state, action)\n","                next_value = self.get_value(next_state)\n","                value_list.append((reward + self.discount_factor * next_value))\n","            # 최댓값을 다음 가치 함수로 대입\n","            next_value_table[state[0]][state[1]] = round(max(value_list), 2)\n","        self.value_table = next_value_table\n","\n","    # 현재 가치 함수로부터 행동을 반환\n","    def get_action(self, state):\n","        action_list = []\n","        max_value = -99999\n","\n","        if state == [2, 2]:\n","            return []\n","\n","        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n","        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n","        for action in self.env.possible_actions:\n","\n","            next_state = self.env.state_after_action(state, action)\n","            reward = self.env.get_reward(state, action)\n","            next_value = self.get_value(next_state)\n","            value = (reward + self.discount_factor * next_value)\n","\n","            if value > max_value:\n","                action_list.clear()\n","                action_list.append(action)\n","                max_value = value\n","            elif value == max_value:\n","                action_list.append(action)\n","\n","        return action_list\n","\n","    def get_value(self, state):\n","        return round(self.value_table[state[0]][state[1]], 2)\n","\n","if __name__ == \"__main__\":\n","    env = Env()\n","    value_iteration = ValueIteration(env)\n","    grid_world = GraphicDisplay(value_iteration)\n","    grid_world.mainloop()"],"metadata":{"id":"5DISNNrlqznn"},"execution_count":null,"outputs":[]}]}