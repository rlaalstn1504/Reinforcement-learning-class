{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwTSusBLbqfrsHajJGkJ0v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Q-Learning"],"metadata":{"id":"84kFfRvmJAeE"}},{"cell_type":"markdown","source":["Q-Learning은 강화학습의 대표적인 알고리즘 중 하나로, 환경과의 상호작용을 통해 최적의 정책을 학습하는 방법입니다.\n","\n","<img src=\"https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig1/AS:1067993664589824@1631640940747/Q-Learning-vs-Deep-Q-Learning.ppm\" width=\"600\">\n","\n","[이미지 출처] https://www.researchgate.net/figure/Q-Learning-vs-Deep-Q-Learning_fig1_351884746\n","\n","## 핵심 개념\n","\n","- **Q-함수**: 상태-행동 가치 함수라고도 불리며 상태(s)에서 행동(a)을 취했을 때의 예상 가치를 리턴하는  함수. 벨만 방정식을 기반으로 Q값을 갱신함\n","- **벨만 방정식**: Q(s,a) = r + γ * max(Q(s',a'))\n","  - Q($s_t$,$a_t$) : 현재 상태 $s_t$에서 행동$a_t$를 했을 때 기대되는 누적 보상\n","  - r: 즉각적인 보상\n","  - γ: 할인 계수\n","  - s': 다음 상태\n","  - a': 다음 상태에서 가능한 모든 행동\n","- **Q-함수 업데이트 식**:\n","  - $$\n","  Q(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha \\big(r + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\big)\n","  $$\n","  - $\\alpha$: 학습률 (0~1 사이의 값으로, 새로운 정보의 반영 정도를 조절)\n","  - 업데이트는 현재 $Q(s_t, a_t)$ 값을 보상과 다음 상태의 최댓값을 반영하여 조금씩 개선하는 방식으로 진행됩니다.\n","\n","- **ε-greedy 정책**: 탐험(exploration)과 활용(exploitation)의 균형을 위한 정책\n","  - Exploration (탐험): 아직 시도해보지 않은 행동을 선택하여 더 나은 보상을 찾는 과정.새로운 정보를 얻을 수 있지만, 단기적으로는 보상이 낮을 수 있음.\n","\n","  - Exploitation (활용): 이미 알고 있는 정보에서 가장 높은 보상을 줄 것이라고 예상되는 행동을 선택. 현재의 지식으로 최대 보상을 얻으려는 접근. ε-Greedy 정책은 이 두 과정을 아래와 같은 방식으로 혼합합니다:\n","  \n","  - 확률 ε로 랜덤하게 행동을 선택(탐험).**확률 (1−𝜀)**로 현재 가장 높은 보상을 줄 것으로 예상되는 행동 선택(활용)\n","\n","\n","\n","## 알고리즘 단계\n","\n","1. Q-테이블 초기화\n","2. 현재 상태 관찰\n","3. ε-greedy 정책에 따라 행동 선택\n","4. 행동 수행 및 보상 획득\n","5. Q-함수 업데이트\n","6. 새로운 상태로 이동\n","7. 2-6 단계 반복\n","\n","## 장단점\n","\n","### 장점:\n","- 모델 없이 학습 가능 (모델 프리 접근법)\n","- 학습 종료 시 수렴성 보장\n","\n","### 단점:\n","- 큰 상태 공간에서는 비효율적\n","- 연속적인 상태나 행동 처리에 제한\n","  - 1. 메모리 요구량 증가 : 큰 상태 공간에서는 Q-테이블의 크기가 기하급수적으로 증가합니다. 각 상태-행동 쌍에 대한 Q-값을 저장해야 하므로, 상태의 수가 증가할수록 필요한 메모리가 급격히 늘어납니다.\n","  - 2. 학습 시간 증가 : 상태 공간이 커질수록 모든 상태-행동 쌍을 충분히 탐색하고 Q-값을 업데이트하는 데 필요한 시간이 크게 증가합니다. 이는 학습 속도를 현저히 저하시킵니다.\n","  - 3. 탐색-활용 딜레마 : 큰 상태 공간에서는 모든 상태를 충분히 탐색하기 어려워집니다. 이로 인해 최적 정책을 찾는 데 필요한 탐색과 학습된 정책을 활용하는 것 사이의 균형을 맞추기가 더욱 어려워집니다.\n","\n","## 코드 설명\n","\n","제공된 코드는 Q-Learning을 구현한 예제입니다. `Env` 클래스는 그리드 월드 환경을, `QLearningAgent` 클래스는 Q-Learning 에이전트를 구현합니다.\n","\n","### 주요 함수 설명\n","\n","1. `Env` 클래스:\n","   - `__init__`: 환경 초기화\n","   - `step`: 행동 수행 및 다음 상태, 보상 반환\n","   - `reset`: 환경 초기화\n","\n","2. `QLearningAgent` 클래스:\n","   - `learn`: Q-함수 업데이트\n","   - `get_action`: ε-greedy 정책에 따른 행동 선택\n","\n","### 코드 실행 과정\n","\n","1. 환경과 에이전트 초기화\n","2. 에피소드 반복:\n","   - 상태 초기화\n","   - 행동 선택 및 수행\n","   - Q-함수 업데이트\n","   - 새로운 상태로 이동\n","   - Q-값 출력\n","3. 1000 에피소드 동안 반복\n","\n","이 코드를 통해 에이전트는 그리드 월드에서 장애물을 피해 목표에 도달하는 최적 경로를 학습합니다."],"metadata":{"id":"ZsmxIpJb7bXE"}},{"cell_type":"markdown","source":["* 아래 코드는 코랩 환경이 아닌 로컬 파이썬 환경에서 진행해주세요!\n","* 코드 출처 : https://github.com/rlcode/reinforcement-learning-kr"],"metadata":{"id":"clS6LSCn_kL3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4ECiNi03ziV"},"outputs":[],"source":["# 필요한 라이브러리 임포트\n","import time  # 딜레이를 추가하거나 시간 측정을 위해 사용\n","import numpy as np  # 배열 및 수학적 계산에 사용\n","import tkinter as tk  # GUI 환경을 구성하기 위한 라이브러리\n","from PIL import ImageTk, Image  # 이미지를 tkinter에서 사용하기 위해 변환\n","\n","# 랜덤 시드 설정 (재현 가능성 보장)\n","np.random.seed(1)\n","\n","# tkinter에서 사용할 이미지 객체\n","PhotoImage = ImageTk.PhotoImage\n","\n","# 그리드월드 크기 설정\n","UNIT = 100  # 각 셀의 크기 (픽셀)\n","HEIGHT = 5  # 그리드월드의 세로 크기 (셀 단위)\n","WIDTH = 5  # 그리드월드의 가로 크기 (셀 단위)\n","\n","\n","class Env(tk.Tk):\n","    def __init__(self):\n","        # tkinter의 Tk 클래스를 초기화\n","        super(Env, self).__init__()\n","        # 환경에서 사용할 행동들 (상, 하, 좌, 우)\n","        self.action_space = ['u', 'd', 'l', 'r']\n","        self.n_actions = len(self.action_space)  # 행동의 개수\n","        self.title('Q Learning')  # 윈도우 제목\n","        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT))  # 윈도우 크기\n","        self.shapes = self.load_images()  # 사용할 이미지 로드\n","        self.canvas = self._build_canvas()  # 캔버스 초기화\n","        self.texts = []  # 그리드 안에 텍스트 표시를 위한 리스트\n","\n","    def _build_canvas(self):\n","        # 캔버스 생성 (그리드 및 이미지 추가)\n","        canvas = tk.Canvas(self, bg='white',\n","                           height=HEIGHT * UNIT,\n","                           width=WIDTH * UNIT)\n","        # 세로선 그리기\n","        for c in range(0, WIDTH * UNIT, UNIT):\n","            x0, y0, x1, y1 = c, 0, c, HEIGHT * UNIT\n","            canvas.create_line(x0, y0, x1, y1)\n","        # 가로선 그리기\n","        for r in range(0, HEIGHT * UNIT, UNIT):\n","            x0, y0, x1, y1 = 0, r, HEIGHT * UNIT, r\n","            canvas.create_line(x0, y0, x1, y1)\n","\n","        # 캔버스에 이미지 추가\n","        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])  # 빨간 네모 (에이전트)\n","        self.triangle1 = canvas.create_image(250, 150, image=self.shapes[1])  # 장애물1\n","        self.triangle2 = canvas.create_image(150, 250, image=self.shapes[1])  # 장애물2\n","        self.circle = canvas.create_image(250, 250, image=self.shapes[2])  # 목표 지점\n","\n","        canvas.pack()  # 캔버스를 tkinter 윈도우에 추가\n","        return canvas\n","\n","    def load_images(self):\n","        # 이미지를 로드하고 크기를 조정하여 반환\n","        rectangle = PhotoImage(\n","            Image.open(\"../img/rectangle.png\").resize((65, 65)))\n","        triangle = PhotoImage(\n","            Image.open(\"../img/triangle.png\").resize((65, 65)))\n","        circle = PhotoImage(\n","            Image.open(\"../img/circle.png\").resize((65, 65)))\n","\n","        return rectangle, triangle, circle  # 로드된 이미지 반환\n","\n","    def text_value(self, row, col, contents, action, font='Helvetica', size=10,\n","                   style='normal', anchor=\"nw\"):\n","        # 그리드 안에 Q 값 표시\n","        if action == 0:  # 상\n","            origin_x, origin_y = 7, 42\n","        elif action == 1:  # 하\n","            origin_x, origin_y = 85, 42\n","        elif action == 2:  # 좌\n","            origin_x, origin_y = 42, 5\n","        else:  # 우\n","            origin_x, origin_y = 42, 77\n","\n","        # 텍스트의 좌표 계산\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def print_value_all(self, q_table):\n","        # 현재 Q 테이블 값을 캔버스에 표시\n","        for i in self.texts:\n","            self.canvas.delete(i)\n","        self.texts.clear()  # 이전 텍스트 지우기\n","        for i in range(HEIGHT):\n","            for j in range(WIDTH):\n","                for action in range(0, 4):\n","                    state = [i, j]\n","                    if str(state) in q_table.keys():\n","                        temp = q_table[str(state)][action]\n","                        self.text_value(j, i, round(temp, 2), action)\n","\n","    def coords_to_state(self, coords):\n","        # 캔버스 좌표를 그리드월드의 상태로 변환\n","        x = int((coords[0] - 50) / 100)\n","        y = int((coords[1] - 50) / 100)\n","        return [x, y]\n","\n","    def state_to_coords(self, state):\n","        # 그리드월드 상태를 캔버스 좌표로 변환\n","        x = int(state[0] * 100 + 50)\n","        y = int(state[1] * 100 + 50)\n","        return [x, y]\n","\n","    def reset(self):\n","        # 환경을 초기 상태로 리셋\n","        self.update()\n","        time.sleep(0.5)  # 딜레이 추가\n","        x, y = self.canvas.coords(self.rectangle)  # 에이전트의 현재 좌표\n","        self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)  # 초기 위치로 이동\n","        self.render()  # 환경 시각화\n","        return self.coords_to_state(self.canvas.coords(self.rectangle))  # 초기 상태 반환\n","\n","    def step(self, action):\n","        # 주어진 행동에 따라 환경의 상태를 변화시키고 보상을 반환\n","        state = self.canvas.coords(self.rectangle)  # 현재 에이전트 좌표\n","        base_action = np.array([0, 0])  # 이동 방향 초기화\n","        self.render()  # 환경 시각화\n","\n","        # 행동에 따른 이동 방향 결정\n","        if action == 0:  # 상\n","            if state[1] > UNIT:\n","                base_action[1] -= UNIT\n","        elif action == 1:  # 하\n","            if state[1] < (HEIGHT - 1) * UNIT:\n","                base_action[1] += UNIT\n","        elif action == 2:  # 좌\n","            if state[0] > UNIT:\n","                base_action[0] -= UNIT\n","        elif action == 3:  # 우\n","            if state[0] < (WIDTH - 1) * UNIT:\n","                base_action[0] += UNIT\n","\n","        # 에이전트 이동\n","        self.canvas.move(self.rectangle, base_action[0], base_action[1])\n","        self.canvas.tag_raise(self.rectangle)  # 에이전트를 맨 위로 배치\n","        next_state = self.canvas.coords(self.rectangle)  # 다음 상태\n","\n","        # 보상 함수\n","        if next_state == self.canvas.coords(self.circle):  # 목표 도달\n","            reward = 100\n","            done = True\n","        elif next_state in [self.canvas.coords(self.triangle1),\n","                            self.canvas.coords(self.triangle2)]:  # 장애물 도달\n","            reward = -100\n","            done = True\n","        else:  # 이동만 한 경우\n","            reward = 0\n","            done = False\n","\n","        next_state = self.coords_to_state(next_state)  # 좌표를 상태로 변환\n","        return next_state, reward, done  # 다음 상태, 보상, 종료 여부 반환\n","\n","    def render(self):\n","        # 환경 시각화 및 딜레이 추가\n","        time.sleep(0.03)\n","        self.update()\n"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","from environment import Env  # 그리드 월드 환경을 정의하는 사용자 정의 클래스\n","from collections import defaultdict  # 기본값이 있는 딕셔너리 생성에 사용\n","\n","class QLearningAgent:\n","    def __init__(self, actions):\n","        # Q-Learning 에이전트를 초기화\n","        # 행동(actions): [0, 1, 2, 3] 순서대로 상, 하, 좌, 우를 의미\n","        self.actions = actions  # 에이전트가 선택할 수 있는 행동 리스트\n","        self.learning_rate = 0.01  # 학습률 α: 새 정보 반영 정도\n","        self.discount_factor = 0.9  # 할인 계수 γ: 미래 보상의 중요도\n","        self.epsilon = 0.9  # 탐험 확률 ε: 무작위 행동 선택 비율\n","        # Q 테이블 초기화: 상태별로 [0.0, 0.0, 0.0, 0.0] 초기값을 가지는 딕셔너리\n","        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n","\n","    # <s, a, r, s'> 샘플로부터 Q-함수를 업데이트\n","    def learn(self, state, action, reward, next_state):\n","        # 현재 상태(state)와 행동(action)에 대한 Q 값\n","        q_1 = self.q_table[state][action]\n","        # 벨만 최적 방정식 기반으로 업데이트 대상 Q 값 계산\n","        # reward + γ * max(Q(s', a')): 현재 보상과 다음 상태에서의 최대 Q 값\n","        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n","        # Q 값 업데이트: 기존 Q 값에 학습률을 곱한 TD 오차를 더함\n","        self.q_table[state][action] += self.learning_rate * (q_2 - q_1)\n","\n","    # Q-테이블 기반의 ε-탐욕 정책으로 행동 선택\n","    def get_action(self, state):\n","        if np.random.rand() < self.epsilon:\n","            # 탐험(Exploration): 무작위 행동 선택\n","            action = np.random.choice(self.actions)\n","        else:\n","            # 활용(Exploitation): Q-테이블에서 가장 높은 값을 가진 행동 선택\n","            state_action = self.q_table[state]\n","            action = self.arg_max(state_action)\n","        return action\n","\n","    @staticmethod\n","    def arg_max(state_action):\n","        # Q 값이 최대인 행동을 반환\n","        max_index_list = []  # 최대 Q 값을 가진 행동들의 인덱스 리스트\n","        max_value = state_action[0]  # 첫 번째 값을 초기 최대값으로 설정\n","        for index, value in enumerate(state_action):\n","            if value > max_value:\n","                # 새로운 최대값 발견 시 리스트 초기화 후 추가\n","                max_index_list.clear()\n","                max_value = value\n","                max_index_list.append(index)\n","            elif value == max_value:\n","                # 최대값과 같은 값을 가진 행동 추가\n","                max_index_list.append(index)\n","        return random.choice(max_index_list)  # 최대값 행동 중 무작위로 선택\n","\n","if __name__ == \"__main__\":\n","    env = Env()  # 사용자 정의 환경 초기화\n","    agent = QLearningAgent(actions=list(range(env.n_actions)))  # 에이전트 초기화\n","\n","    for episode in range(1000):  # 1000개의 에피소드 동안 학습 반복\n","        state = env.reset()  # 환경 초기화 및 초기 상태 반환\n","\n","        while True:  # 한 에피소드 동안 반복\n","            env.render()  # 환경 시각화 (현재 상태 출력)\n","\n","            # 현재 상태(state)에 따른 행동(action) 선택\n","            action = agent.get_action(str(state))\n","            # 행동 수행 후 다음 상태(next_state), 보상(reward), 종료 여부(done) 반환\n","            next_state, reward, done = env.step(action)\n","\n","            # Q-함수 업데이트: <s, a, r, s'> 샘플로 학습\n","            agent.learn(str(state), action, reward, str(next_state))\n","            state = next_state  # 상태 업데이트\n","\n","            # 모든 상태-행동에 대한 Q 값을 화면에 표시\n","            env.print_value_all(agent.q_table)\n","\n","            if done:  # 에피소드 종료 조건\n","                break\n"],"metadata":{"id":"YNcuFrNh_3Jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실습 문제 1: ϵ-Decay 를 적용하여 시간이 지날수록 𝜖 이 줄어들도록 설정하세요."],"metadata":{"id":"5_cj_kq5SEKI"},"execution_count":null,"outputs":[]}]}