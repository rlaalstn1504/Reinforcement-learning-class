{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMv+EcIliJDsHmHNJVl3cQA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Q-Learning"],"metadata":{"id":"84kFfRvmJAeE"}},{"cell_type":"markdown","source":["Q-Learning은 강화학습의 대표적인 알고리즘 중 하나로, 환경과의 상호작용을 통해 최적의 정책을 학습하는 방법입니다.\n","\n","<img src=\"https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig1/AS:1067993664589824@1631640940747/Q-Learning-vs-Deep-Q-Learning.ppm\" width=\"600\">\n","\n","[이미지 출처] https://www.researchgate.net/figure/Q-Learning-vs-Deep-Q-Learning_fig1_351884746\n","\n","## 핵심 개념\n","\n","- **Q-함수**: 상태(s)에서 행동(a)을 취했을 때의 예상 보상을 나타내는 함수\n","- **벨만 방정식**: Q(s,a) = r + γ * max(Q(s',a'))\n","  - r: 즉각적인 보상\n","  - γ: 할인 계수\n","  - s': 다음 상태\n","  - a': 다음 상태에서 가능한 모든 행동\n","- **ε-greedy 정책**: 탐험(exploration)과 활용(exploitation)의 균형을 위한 정책\n","\n","## 알고리즘 단계\n","\n","1. Q-테이블 초기화\n","2. 현재 상태 관찰\n","3. ε-greedy 정책에 따라 행동 선택\n","4. 행동 수행 및 보상 획득\n","5. Q-함수 업데이트\n","6. 새로운 상태로 이동\n","7. 2-6 단계 반복\n","\n","## 장단점\n","\n","### 장점:\n","- 모델 없이 학습 가능\n","- 수렴성 보장\n","\n","### 단점:\n","- 큰 상태 공간에서는 비효율적\n","- 연속적인 상태나 행동 처리에 제한\n","  - 1. 메모리 요구량 증가 : 큰 상태 공간에서는 Q-테이블의 크기가 기하급수적으로 증가합니다. 각 상태-행동 쌍에 대한 Q-값을 저장해야 하므로, 상태의 수가 증가할수록 필요한 메모리가 급격히 늘어납니다.\n","  - 2. 학습 시간 증가 : 상태 공간이 커질수록 모든 상태-행동 쌍을 충분히 탐색하고 Q-값을 업데이트하는 데 필요한 시간이 크게 증가합니다. 이는 학습 속도를 현저히 저하시킵니다.\n","  - 3. 탐색-활용 딜레마 : 큰 상태 공간에서는 모든 상태를 충분히 탐색하기 어려워집니다. 이로 인해 최적 정책을 찾는 데 필요한 탐색과 학습된 정책을 활용하는 것 사이의 균형을 맞추기가 더욱 어려워집니다.\n","\n","## 코드 설명\n","\n","제공된 코드는 Q-Learning을 구현한 예제입니다. `Env` 클래스는 그리드 월드 환경을, `QLearningAgent` 클래스는 Q-Learning 에이전트를 구현합니다.\n","\n","### 주요 함수 설명\n","\n","1. `Env` 클래스:\n","   - `__init__`: 환경 초기화\n","   - `step`: 행동 수행 및 다음 상태, 보상 반환\n","   - `reset`: 환경 초기화\n","\n","2. `QLearningAgent` 클래스:\n","   - `learn`: Q-함수 업데이트\n","   - `get_action`: ε-greedy 정책에 따른 행동 선택\n","\n","### 코드 실행 과정\n","\n","1. 환경과 에이전트 초기화\n","2. 에피소드 반복:\n","   - 상태 초기화\n","   - 행동 선택 및 수행\n","   - Q-함수 업데이트\n","   - 새로운 상태로 이동\n","   - Q-값 출력\n","3. 1000 에피소드 동안 반복\n","\n","이 코드를 통해 에이전트는 그리드 월드에서 장애물을 피해 목표에 도달하는 최적 경로를 학습합니다."],"metadata":{"id":"ZsmxIpJb7bXE"}},{"cell_type":"markdown","source":["* 아래 코드는 코랩 환경이 아닌 로컬 파이썬 환경에서 진행해주세요!\n","* 코드 출처 : https://github.com/rlcode/reinforcement-learning-kr"],"metadata":{"id":"clS6LSCn_kL3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4ECiNi03ziV"},"outputs":[],"source":["# environment.py\n","import time\n","import numpy as np\n","import tkinter as tk\n","from PIL import ImageTk, Image\n","\n","np.random.seed(1)\n","PhotoImage = ImageTk.PhotoImage\n","UNIT = 100  # 픽셀 수\n","HEIGHT = 5  # 그리드월드 세로\n","WIDTH = 5  # 그리드월드 가로\n","\n","\n","class Env(tk.Tk):\n","    def __init__(self):\n","        super(Env, self).__init__()\n","        self.action_space = ['u', 'd', 'l', 'r']\n","        self.n_actions = len(self.action_space)\n","        self.title('Q Learning')\n","        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT))\n","        self.shapes = self.load_images()\n","        self.canvas = self._build_canvas()\n","        self.texts = []\n","\n","    def _build_canvas(self):\n","        canvas = tk.Canvas(self, bg='white',\n","                           height=HEIGHT * UNIT,\n","                           width=WIDTH * UNIT)\n","        # 그리드 생성\n","        for c in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = c, 0, c, HEIGHT * UNIT\n","            canvas.create_line(x0, y0, x1, y1)\n","        for r in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n","            x0, y0, x1, y1 = 0, r, HEIGHT * UNIT, r\n","            canvas.create_line(x0, y0, x1, y1)\n","\n","        # 캔버스에 이미지 추가\n","        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])\n","        self.triangle1 = canvas.create_image(250, 150, image=self.shapes[1])\n","        self.triangle2 = canvas.create_image(150, 250, image=self.shapes[1])\n","        self.circle = canvas.create_image(250, 250, image=self.shapes[2])\n","\n","        canvas.pack()\n","\n","        return canvas\n","\n","    def load_images(self):\n","        rectangle = PhotoImage(\n","            Image.open(\"../img/rectangle.png\").resize((65, 65)))\n","        triangle = PhotoImage(\n","            Image.open(\"../img/triangle.png\").resize((65, 65)))\n","        circle = PhotoImage(\n","            Image.open(\"../img/circle.png\").resize((65, 65)))\n","\n","        return rectangle, triangle, circle\n","\n","    def text_value(self, row, col, contents, action, font='Helvetica', size=10,\n","                   style='normal', anchor=\"nw\"):\n","\n","        if action == 0:\n","            origin_x, origin_y = 7, 42\n","        elif action == 1:\n","            origin_x, origin_y = 85, 42\n","        elif action == 2:\n","            origin_x, origin_y = 42, 5\n","        else:\n","            origin_x, origin_y = 42, 77\n","\n","        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n","        font = (font, str(size), style)\n","        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n","                                       font=font, anchor=anchor)\n","        return self.texts.append(text)\n","\n","    def print_value_all(self, q_table):\n","        for i in self.texts:\n","            self.canvas.delete(i)\n","        self.texts.clear()\n","        for i in range(HEIGHT):\n","            for j in range(WIDTH):\n","                for action in range(0, 4):\n","                    state = [i, j]\n","                    if str(state) in q_table.keys():\n","                        temp = q_table[str(state)][action]\n","                        self.text_value(j, i, round(temp, 2), action)\n","\n","    def coords_to_state(self, coords):\n","        x = int((coords[0] - 50) / 100)\n","        y = int((coords[1] - 50) / 100)\n","        return [x, y]\n","\n","    def state_to_coords(self, state):\n","        x = int(state[0] * 100 + 50)\n","        y = int(state[1] * 100 + 50)\n","        return [x, y]\n","\n","    def reset(self):\n","        self.update()\n","        time.sleep(0.5)\n","        x, y = self.canvas.coords(self.rectangle)\n","        self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n","        self.render()\n","        return self.coords_to_state(self.canvas.coords(self.rectangle))\n","\n","    def step(self, action):\n","        state = self.canvas.coords(self.rectangle)\n","        base_action = np.array([0, 0])\n","        self.render()\n","\n","        if action == 0:  # 상\n","            if state[1] > UNIT:\n","                base_action[1] -= UNIT\n","        elif action == 1:  # 하\n","            if state[1] < (HEIGHT - 1) * UNIT:\n","                base_action[1] += UNIT\n","        elif action == 2:  # 좌\n","            if state[0] > UNIT:\n","                base_action[0] -= UNIT\n","        elif action == 3:  # 우\n","            if state[0] < (WIDTH - 1) * UNIT:\n","                base_action[0] += UNIT\n","\n","        # 에이전트 이동\n","        self.canvas.move(self.rectangle, base_action[0], base_action[1])\n","        # 에이전트(빨간 네모)를 가장 상위로 배치\n","        self.canvas.tag_raise(self.rectangle)\n","        next_state = self.canvas.coords(self.rectangle)\n","\n","        # 보상 함수\n","        if next_state == self.canvas.coords(self.circle):\n","            reward = 100\n","            done = True\n","        elif next_state in [self.canvas.coords(self.triangle1),\n","                            self.canvas.coords(self.triangle2)]:\n","            reward = -100\n","            done = True\n","        else:\n","            reward = 0\n","            done = False\n","\n","        next_state = self.coords_to_state(next_state)\n","        return next_state, reward, done\n","\n","    def render(self):\n","        time.sleep(0.03)\n","        self.update()"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","from environment import Env\n","from collections import defaultdict\n","\n","class QLearningAgent:\n","    def __init__(self, actions):\n","        # 행동 = [0, 1, 2, 3] 순서대로 상, 하, 좌, 우\n","        self.actions = actions\n","        self.learning_rate = 0.01\n","        self.discount_factor = 0.9\n","        self.epsilon = 0.9\n","        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n","\n","    # <s, a, r, s'> 샘플로부터 큐함수 업데이트\n","    def learn(self, state, action, reward, next_state):\n","        q_1 = self.q_table[state][action]\n","        # 벨만 최적 방정식을 사용한 큐함수의 업데이트\n","        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n","        self.q_table[state][action] += self.learning_rate * (q_2 - q_1)\n","\n","    # 큐함수에 의거하여 입실론 탐욕 정책에 따라서 행동을 반환\n","    def get_action(self, state):\n","        if np.random.rand() < self.epsilon:\n","            # 무작위 행동 반환\n","            action = np.random.choice(self.actions)\n","        else:\n","            # 큐함수에 따른 행동 반환\n","            state_action = self.q_table[state]\n","            action = self.arg_max(state_action)\n","        return action\n","\n","    @staticmethod\n","    def arg_max(state_action):\n","        max_index_list = []\n","        max_value = state_action[0]\n","        for index, value in enumerate(state_action):\n","            if value > max_value:\n","                max_index_list.clear()\n","                max_value = value\n","                max_index_list.append(index)\n","            elif value == max_value:\n","                max_index_list.append(index)\n","        return random.choice(max_index_list)\n","\n","if __name__ == \"__main__\":\n","    env = Env()\n","    agent = QLearningAgent(actions=list(range(env.n_actions)))\n","\n","    for episode in range(1000):\n","        state = env.reset()\n","\n","        while True:\n","            env.render()\n","\n","            # 현재 상태에 대한 행동 선택\n","            action = agent.get_action(str(state))\n","            # 행동을 취한 후 다음 상태, 보상 에피소드의 종료여부를 받아옴\n","            next_state, reward, done = env.step(action)\n","\n","            # <s,a,r,s'>로 큐함수를 업데이트\n","            agent.learn(str(state), action, reward, str(next_state))\n","            state = next_state\n","            # 모든 큐함수를 화면에 표시\n","            env.print_value_all(agent.q_table)\n","\n","            if done:\n","                break"],"metadata":{"id":"YNcuFrNh_3Jg"},"execution_count":null,"outputs":[]}]}