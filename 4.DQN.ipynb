{"cells":[{"cell_type":"markdown","source":["### 딥러닝과 강화학습의 융합\n","1. **강화학습(RL)**: 에이전트가 환경과 상호작용하며 최적의 행동을 학습하는 과정.\n","   - 핵심 요소:\n","     - **상태(State)**: 현재 환경의 상태.\n","     - **행동(Action)**: 에이전트가 취할 수 있는 행동.\n","     - **보상(Reward)**: 행동의 결과로 환경이 에이전트에 제공하는 피드백.\n","     - **정책(Policy)**: 상태에 따라 행동을 결정하는 함수.\n","   - 목표: 보상을 최대로 만드는 정책 학습.\n","\n","2. **딥러닝(DL)**: 심층 신경망을 사용하여 복잡한 패턴이나 함수를 모델링.\n","   - 강화학습의 비선형 함수 근사를 위해 딥러닝이 사용됨.\n","\n","3. **심층 강화학습(DRL)**: 딥러닝과 강화학습의 융합.\n","   - Q-값, 정책 함수 등을 심층 신경망으로 근사.\n","   - 복잡한 환경에서도 학습 가능."],"metadata":{"id":"BbpdpMP-Jiov"}},{"cell_type":"markdown","source":["### DQN (Deep Q-Network) 소개\n","- 전통적인 Q-Learning은 상태 공간이 클 경우 Q-테이블을 저장하기 어렵고, 일반화가 어려움.\n","- **DQN**은 심층 신경망(Deep Neural Network)을 사용하여 Q-값을 근사:\n","  - 입력: 상태(state).\n","  - 출력: 행동(action)에 대한 Q-값.\n","\n","#### 주요 특징\n","1. **경험 재생(Experience Replay)**:\n","   - 에이전트가 경험한 데이터를 저장하여 랜덤 샘플링으로 학습.\n","   - 데이터 간 상관성을 줄이고 학습 효율 향상.\n","\n","2. **타겟 네트워크(Target Network)**:\n","   - Q-값 업데이트 안정성을 위해 메인 네트워크와 별도로 고정된 타겟 네트워크 사용.\n","   - 일정 간격으로 타겟 네트워크를 메인 네트워크의 가중치로 갱신."],"metadata":{"id":"nmMA13RrJmLr"}},{"cell_type":"markdown","source":["## **1. Q-Learning 업데이트 수식**\n","\n","기존 Q-Learning에서는 다음과 같은 수식을 사용합니다:\n","\n","$$\n","Q(s, a) $\\leftarrow Q(s, a) + \\alpha \\Big(r + \\gamma $max_{a'} Q(s', a') - Q(s, a) $\\Big)\n","$$\n","\n","- **$Q(s, a)$**: 상태 $( s $)에서 행동 $( a $)를 했을 때의 Q-값 (예상 보상)\n","- **$r $**: 현재 행동 $( a $)를 통해 받은 보상\n","- **$\\gamma$**: 할인율 (미래 보상을 현재 가치로 반영할 비율)\n","- **$\\alpha$**: 학습률 (새로운 값과 기존 값의 반영 비율)\n","- **$\\max_{a'} Q(s', a') $**: 다음 상태 $( s' $)에서 가능한 행동 중 가장 큰 Q-값"],"metadata":{"id":"62zamTaaA4rv"}},{"cell_type":"markdown","source":["## **2. DQN의 업데이트 수식**\n","\n","DQN은 심층 신경망(Deep Neural Network)을 사용하여 \\( Q \\)-값을 근사합니다. 업데이트를 위해 다음 **손실 함수**(Loss Function)를 사용합니다:\n","\n","$$\n","\\text{Loss} = \\Big( r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a') - Q_{\\text{main}}(s, a) \\Big)^2\n","$$\n","\n","- **$Q_{\\text{main}}(s, a)$**: 메인 네트워크에서 예측한 Q-값\n","- **$Q_{\\text{target}}(s', a')$**: 타겟 네트워크에서 계산한 Q-값 (고정된 값 사용)\n","- **$r$**: 현재 보상\n","- **$\\gamma \\max_{a'} Q_{\\text{target}}(s', a')$**: 미래 보상의 예상치\n","\n","이 손실 함수를 최소화하도록 메인 네트워크가 학습됩니다."],"metadata":{"id":"LX26s2rpA4hB"}},{"cell_type":"markdown","source":["## **3. 경험 재생 (Experience Replay)**\n","\n","경험 재생은 DQN의 중요한 구성 요소 중 하나입니다. 이를 통해 신경망 학습의 안정성을 높입니다.\n","\n","### **개념 설명**:\n","- 에이전트는 환경과 상호작용하면서 경험을 만듭니다. 각 경험은 다음과 같은 형태로 저장됩니다:  \n","$(s, a, r, s', \\text{done})$\n","  - **$ s $**: 현재 상태\n","  - **$ a $**: 행동\n","  - **$ r $**: 보상\n","  - **$ s' $**: 다음 상태\n","  - **$\\text{done}$**: 에피소드 종료 여부 (True/False)\n","\n","- 이러한 경험을 모두 **메모리 버퍼**에 저장해 두고, 학습할 때 **랜덤하게** 추출하여 사용합니다.\n","\n","### **왜 경험 재생이 필요한가?**  \n","1. **데이터 상관성 제거**:  \n","   에이전트가 연속된 데이터를 사용하면 데이터 간 상관관계가 높아져 학습이 어려워집니다. 랜덤 샘플링을 통해 이를 방지합니다.  \n","2. **데이터 재사용**:  \n","   한 번의 경험을 여러 번 학습에 사용하여 데이터 효율성을 높입니다.  \n","\n","### **쉽게 비유하면**:\n","경험 재생은 **노트를 작성하고 복습하는 것**과 비슷합니다. 즉, 과거 경험을 버리지 않고 모아 두었다가 중요한 순간에 꺼내서 학습하는 방식입니다."],"metadata":{"id":"slTnIlIDA4ek"}},{"cell_type":"markdown","source":["## **4. 타겟 네트워크 (Target Network)**\n","\n","타겟 네트워크는 DQN 학습의 **안정성**을 높이는 기술입니다.\n","\n","### **동작 원리**:\n","- **메인 네트워크**: 현재 정책을 학습하고 최적의 Q-값을 예측합니다.  \n","- **타겟 네트워크**: 메인 네트워크의 가중치를 **일정 주기마다 복사**하여 고정된 Q-값을 제공합니다.\n","\n","### **왜 타겟 네트워크가 필요한가?**\n","- 메인 네트워크와 타겟 네트워크가 동시에 학습되면 \\( Q \\)-값이 **불안정**해질 수 있습니다.  \n","- 타겟 네트워크는 일정 시간 동안 **고정된 Q-값**을 제공하여 학습이 안정적으로 이루어지도록 돕습니다.\n","\n","### **쉽게 비유하면**:\n","타겟 네트워크는 **참고서**와 같습니다. 참고서는 일정 시간 동안 바뀌지 않으므로 학습 목표가 흔들리지 않습니다. 대신 시간이 지나면 최신 정보를 반영해 갱신됩니다."],"metadata":{"id":"RjXyGs_qBFT3"}},{"cell_type":"markdown","metadata":{"id":"38pOBp53w6ah"},"source":["## **5. 전체 구조 정리**\n","\n","DQN의 학습 과정은 다음과 같습니다:\n","1. **환경과 상호작용**하며 경험$( (s, a, r, s',\\text{done})$)를 저장.\n","2. 메모리 버퍼에서 **랜덤 샘플링**을 통해 학습 데이터를 추출.\n","3. 메인 네트워크에서 현재 상태의 Q-값$( Q_{\\text{main}}(s, a)$)를 예측.\n","4. 타겟 네트워크에서 목표 Q-값$(r +\\gamma \\max_{a'} Q_{\\text{target}}(s', a'))$를 계산.\n","5. 손실 함수(Loss)를 계산하고 메인 네트워크를 **업데이트**.\n","6. 일정 주기마다 타겟 네트워크를 메인 네트워크로 **동기화**.\n"]},{"cell_type":"markdown","metadata":{"id":"IVZyTI_oxEsY"},"source":["## DQN 실습\n","\n","간단한 OpenAI Gym 환경에서 DQN 모델을 구현하여 학습하는 과정을 실습합니다.\n","\n","- 환경: `CartPole-v1`.\n","- 목표: 막대가 쓰러지지 않고 균형을 유지하도록 에이전트를 학습.\n","- 주요 구성 요소:\n","  1. 경험 재생(Experience Replay)을 위한 메모리 버퍼.\n","  2. 메인 네트워크와 타겟 네트워크.\n","  3. DQN 학습 루프.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8765,"status":"ok","timestamp":1732299244463,"user":{"displayName":"김민수","userId":"14499279899039145671"},"user_tz":-540},"id":"fyqDYVnuynX_","outputId":"01631f3e-0708-4502-cba8-9f40c2c136c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.36.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.5.1)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.66.6)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.26.4)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.32.3)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (11.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy->colabgymrender) (75.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2024.8.30)\n","Building wheels for collected packages: colabgymrender\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3114 sha256=b44c735d7d88a6b2928a8b36da82e8fc34bb679394d09dc228591127d7bf0885\n","  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n","Successfully built colabgymrender\n","Installing collected packages: colabgymrender\n","Successfully installed colabgymrender-1.1.0\n"]}],"source":["# 아래 코드를 복사하여 로컬 환경에서 실행해주세요\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","from collections import deque\n","import gym\n","\n","# CartPole-v1 환경을 생성하고 초기화합니다. render_mode를 \"human\"으로 설정하여 화면에 표시합니다.\n","env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","state = env.reset()\n","\n","# 상태 공간의 크기와 행동 공간의 크기를 정의합니다.\n","state_size = env.observation_space.shape[0]  # 상태의 차원 (카트의 위치, 속도, 막대의 각도, 각속도)\n","action_size = env.action_space.n  # 가능한 행동의 수 (왼쪽으로 밀기, 오른쪽으로 밀기)\n","\n","# 경험 재생을 위한 버퍼 클래스를 정의합니다.\n","# 경험 재생(Experience Replay)은 강화학습에서 중요한 기법으로,\n","# 에이전트가 환경과 상호작용하며 얻은 경험(상태, 행동, 보상, 다음 상태)을 저장하고\n","# 나중에 무작위로 샘플링하여 학습에 사용합니다. 이를 통해:\n","# 1. 데이터의 시간적 연관성을 줄여 학습의 안정성을 높입니다.\n","# 2. 과거의 경험을 재사용하여 데이터 효율성을 증가시킵니다.\n","# 3. 드물게 발생하는 중요한 경험들을 여러 번 학습에 활용할 수 있습니다.\n","class ReplayBuffer:\n","    def __init__(self, max_size=50000):  # 버퍼의 최대 크기를 50000으로 설정\n","        self.buffer = deque(maxlen=max_size)\n","\n","    def add(self, experience):\n","        # 새로운 경험을 버퍼에 추가합니다.\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        # 버퍼에서 무작위로 batch_size만큼의 샘플을 추출합니다.\n","        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","        return [self.buffer[idx] for idx in indices]\n","\n","    def size(self):\n","        # 현재 버퍼의 크기를 반환합니다.\n","        return len(self.buffer)\n","\n","# 신경망 모델을 구축하는 함수를 정의합니다.\n","def build_model():\n","    model = Sequential([\n","        Dense(24, input_dim=state_size, activation='relu'),  # 입력층: 상태 크기\n","        Dense(24, activation='relu'),  # 은닉층\n","        Dense(action_size, activation='linear')  # 출력층: 각 행동에 대한 Q값\n","    ])\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n","    return model\n","\n","# DQN 에이전트 클래스를 정의합니다.\n","class DQNAgent:\n","    def __init__(self):\n","        self.main_model = build_model()  # 주 신경망 모델\n","        self.target_model = build_model()  # 타겟 신경망 모델\n","        self.target_model.set_weights(self.main_model.get_weights())  # 타겟 모델의 가중치를 주 모델과 동일하게 초기화\n","        self.replay_buffer = ReplayBuffer()  # 경험 재생 버퍼\n","        self.gamma = 0.99  # 할인 계수\n","        self.epsilon = 1.0  # 탐험률 초기값\n","        self.epsilon_decay = 0.995  # 탐험률 감소 비율\n","        self.epsilon_min = 0.01  # 최소 탐험률\n","        self.batch_size = 64  # 학습 배치 크기\n","\n","    def update_target_network(self):\n","        # 타겟 네트워크의 가중치를 주 네트워크의 가중치로 업데이트합니다.\n","        self.target_model.set_weights(self.main_model.get_weights())\n","\n","    def select_action(self, state):\n","        # 입실론-그리디 정책에 따라 행동을 선택합니다.\n","        if np.random.rand() <= self.epsilon:\n","            return env.action_space.sample()  # 무작위 행동 선택\n","        q_values = self.main_model.predict(state)\n","        return np.argmax(q_values[0])  # 최대 Q값을 가진 행동 선택\n","\n","    def train(self):\n","        # 경험 재생 버퍼에서 배치를 샘플링하여 학습을 수행합니다.\n","        if self.replay_buffer.size() < self.batch_size:\n","            return\n","\n","        batch = self.replay_buffer.sample(self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*batch)\n","        states = np.array(states).squeeze(axis=1)\n","        next_states = np.array(next_states).squeeze(axis=1)\n","\n","        # 현재 상태에 대한 Q값 예측\n","        target_qs = self.main_model.predict(states)\n","        # 다음 상태에 대한 Q값 예측 (타겟 네트워크 사용)\n","        next_qs = self.target_model.predict(next_states)\n","\n","        # Q-learning 업데이트 규칙을 적용하여 타겟 Q값을 계산\n","        for i in range(self.batch_size):\n","            if dones[i]:\n","                target_qs[i][actions[i]] = rewards[i]\n","            else:\n","                target_qs[i][actions[i]] = rewards[i] + self.gamma * np.max(next_qs[i])\n","\n","        # 주 네트워크를 학습시킵니다.\n","        self.main_model.fit(states, target_qs, epochs=1, verbose=0, batch_size=32)\n","\n","# DQN 에이전트 인스턴스를 생성합니다.\n","agent = DQNAgent()\n","episodes = 500  # 학습할 에피소드 수\n","\n","# 학습 루프\n","for episode in range(episodes):\n","    state = env.reset()\n","    state = state[0] if isinstance(state, tuple) else state  # 상태가 튜플인 경우 첫 번째 요소만 사용\n","    state = np.reshape(state, [1, state_size])  # 상태를 적절한 형태로 변환\n","    total_reward = 0\n","    done = False\n","\n","    while not done:\n","        action = agent.select_action(state)  # 행동 선택\n","\n","        # 환경에서 한 스텝 진행\n","        step_result = env.step(action)\n","        if len(step_result) == 4:  # gym 버전에 따라 반환값이 다를 수 있음\n","            next_state, reward, done, info = step_result\n","        elif len(step_result) == 5:\n","            next_state, reward, done, truncated, info = step_result\n","            done = done or truncated  # truncated도 종료 조건으로 처리\n","        else:\n","            raise ValueError(f\"Unexpected step result length: {len(step_result)}\")\n","\n","        next_state = next_state[0] if isinstance(next_state, tuple) else next_state\n","        next_state = np.reshape(next_state, [1, state_size])  # 다음 상태를 적절한 형태로 변환\n","\n","        # 경험을 리플레이 버퍼에 저장\n","        agent.replay_buffer.add((state, action, reward, next_state, done))\n","\n","        state = next_state\n","        total_reward += reward\n","\n","        # 에이전트 학습\n","        agent.train()\n","\n","    # 10 에피소드마다 타겟 네트워크 업데이트\n","    if episode % 10 == 0:\n","        agent.update_target_network()\n","\n","    # 탐험률 감소\n","    if agent.epsilon > agent.epsilon_min:\n","        agent.epsilon *= agent.epsilon_decay\n","\n","    print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n","\n","# 환경 종료\n","env.close()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO3LemCkkarVxHRJVEIK+0g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}