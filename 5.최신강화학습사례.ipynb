{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8+pebR8NNhCuJT474iHSV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 딥러닝에서 강화학습(RL)이 사용되는 최신 사례\n"],"metadata":{"id":"QD8083TzuAkn"}},{"cell_type":"markdown","source":["## 1.대규모 언어 모델과 RLHF\n","- **RLHF (Reinforcement Learning with Human Feedback)**:\n","  - **사용 사례**:\n","    - ChatGPT (OpenAI): 인간 피드백을 활용해 언어 모델의 응답 품질 향상.\n","    - InstructGPT: 사용자의 지침에 맞는 응답 생성.\n","  - **작동 원리**:\n","    - 인간 평가자가 제공하는 피드백(보상 모델)을 기반으로, 언어 모델이 보상을 극대화하는 방향으로 미세 조정.\n","    - 예: 사용자 선호도가 높은 응답을 더 많이 생성하도록 학습."],"metadata":{"id":"txjmSjsXBvsr"}},{"cell_type":"markdown","source":["## ChatGPT에 적용된 RHLF (Reinforcement Learning with Human Feedback)\n","\n","ChatGPT는 인간의 피드백을 활용해 보다 자연스럽고 유용한 답변을 생성하기 위해 RHLF 과정을 거칩니다. 이 과정은 크게 3단계로 나뉩니다."],"metadata":{"id":"KJGbkeddOjiU"}},{"cell_type":"markdown","source":["### **1단계: Supervised Fine-Tuning (SFT)**\n","- **목적**: 사전 학습된 언어 모델(Pre-trained LM)이 인간이 의도한 정책(policy)을 따르도록 Fine-Tuning합니다.\n","- **방법**:  \n","  - 인간 Labeler(또는 Trainer)가 고품질의 소량 데이터셋을 선별합니다.\n","  - 이 데이터를 사용하여 사전 학습된 모델을 Fine-Tuning합니다.\n","- **결과**: 인간의 의도를 반영한 초기 모델(SFT 모델)이 생성됩니다."],"metadata":{"id":"1-ph-OZbOjfg"}},{"cell_type":"markdown","source":["### **2단계: Reward Model 학습 (Mimic Human Preferences)**\n","- **목적**: 모델의 응답 품질을 평가하는 보상 모델(Reward Model)을 학습합니다.\n","- **방법**:  \n","  - SFT 모델이 생성한 여러 답변 후보를 수집합니다.\n","  - 인간 Labeler가 각 답변에 대해 랭킹을 매기고, 어떤 답변이 더 나은지 점수화한 데이터를 만듭니다.\n","  - 이 랭킹 데이터를 사용해 Reward Model을 학습시킵니다.\n","- **결과**: 인간 선호도를 학습한 보상 모델이 생성됩니다."],"metadata":{"id":"wAYKvEUlOmYF"}},{"cell_type":"markdown","source":["### **3단계: 강화학습 (PPO를 이용한 Fine-Tuning)**\n","- **목적**: SFT 모델을 사용자 피드백과 Reward Model을 활용해 강화합니다.\n","- **방법**:  \n","  - 사용자 입력 데이터를 SFT 모델에 제공합니다.\n","  - SFT 모델이 생성한 답변을 Reward Model과 상호작용하여 평가합니다.\n","  - Proximal Policy Optimization(PPO, 정책을 직접 최적화하는 방식) 알고리즘을 사용해 SFT 모델을 업데이트합니다.\n","- **결과**: 사용자 피드백을 반영한 최적화된 ChatGPT 모델이 완성됩니다."],"metadata":{"id":"MVxHYj8OOqzS"}},{"cell_type":"markdown","source":["### **요약**\n","ChatGPT는 아래의 단계를 통해 인간 피드백을 최대한 반영한 모델로 발전합니다:\n","1. **SFT**: 인간의 의도를 반영한 초기 모델 학습.\n","2. **Reward Model**: 인간 선호도를 반영한 평가 모델 학습.\n","3. **PPO 강화학습**: 사용자 입력과 상호작용하며 최적화.\n","\n","이를 통해 ChatGPT는 더 높은 품질의 응답을 생성할 수 있습니다."],"metadata":{"id":"fBy1epgs8AUU"}},{"cell_type":"markdown","source":["## 2.게임 분야에서의 강화학습 활용 사례와 기술\n","\n","- 강화학습은 게임 분야에서 인공지능(AI) 에이전트의 성능을 혁신적으로 향상시키며, 다양한 방식으로 활용되고 있습니다. 아래는 주요 사례와 적용된 기술들을 정리한 내용입니다."],"metadata":{"id":"2O36EBHhOGp4"}},{"cell_type":"markdown","source":["### **1. AlphaGo와 AlphaGo Zero**\n","- **AlphaGo**: DeepMind에서 개발한 AI로, 강화학습을 통해 바둑 게임을 마스터했습니다.  \n","- **AlphaGo Zero**: 인간의 기보 데이터를 사용하지 않고, **자가 학습(self-play)**만으로 최고 수준의 바둑 실력을 달성했습니다.\n","\n","#### 주요 기술:\n","- 심층 신경망을 사용하여 **게임 상태를 평가**하고 **다음 수를 예측**.\n","- 자가 대국(self-play)을 통한 강화학습.\n","- **몬테카를로 트리 탐색(MCTS)** 없이 단일 신경망과 간단한 트리 탐색만으로 구현.\n","\n","\n","<img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677275363/mirroredImages/FF8i6SLfKb4g7C4EL/ppa1lup4f8ydkxsgumxy.gif\" width=\"300\">"],"metadata":{"id":"V7mr-gR6OUd7"}},{"cell_type":"markdown","source":["### **2. Atari 게임 마스터**\n","- Google DeepMind의 **Deep Q-Network (DQN)**은 다양한 **Atari 2600 게임**을 플레이하며, 인간 수준 이상의 성과를 보여주었습니다.\n","\n","#### 주요 기술:\n","- **컨볼루션 신경망(CNN)**을 사용해 **raw 픽셀 입력**으로부터 Q-함수를 학습.\n","- **경험 리플레이(Experience Replay)**를 통한 안정적인 학습.\n","- 동일한 아키텍처와 하이퍼파라미터를 여러 게임에 적용하여 일반화된 성능을 달성."],"metadata":{"id":"ACgWWRumOUcL"}},{"cell_type":"markdown","source":["### **강화학습의 게임 분야 적용 의의**\n","이와 같은 사례들은 강화학습이 게임 AI 개발에 혁신을 가져오고 있음을 보여줍니다. 특히, 다음과 같은 측면에서 강화학습의 잠재력이 입증되고 있습니다:\n","- **복잡한 전략 수립**: 바둑이나 MOBA 게임과 같은 복잡한 환경에서의 의사결정.\n","- **실시간 의사결정**: 플레이어와의 실시간 상호작용.\n","- **적응형 게임플레이**: 플레이어 행동에 맞춘 게임 난이도 조절.\n","\n","강화학습은 게임 AI의 성능과 플레이 경험을 향상시키는 데 중요한 역할을 하고 있습니다."],"metadata":{"id":"SbjrIdMmOPJb"}},{"cell_type":"markdown","source":["## 3.로봇\n","\n","- 로봇들은 수동으로 프로그래밍되는 대신 자율적으로 움직임을 학습했을 때 성능이 크게 향상되었음\n","- 또한 걷기와 축구뿐만 아니라 더 복잡한 운동 기술도 습득가능했음\n","- 로봇의 정교한 움직임을 위해 강화학습이 앞으로도 많이 활용될 것으로 미래 전망됨\n","\n","  <img src=\"https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/05/052124_mh_ai-movement_feat.jpg?resize=1030%2C580&ssl=1\" width=\"500\">\n","\n","  https://www.sciencenews.org/article/reinforcement-learn-ai-humanoid-robots"],"metadata":{"id":"w2Zzj1Uy5Tru"}},{"cell_type":"markdown","source":["## 4.강화학습의 단점 및 한계점\n","\n","- 보상이 드물게 주어지는 환경에서의 학습이 어려움\n","- 먼 미래에 주어지는 불확실한 보상과 가까운 미래의 큰 보상 사이 딜레마\n"],"metadata":{"id":"bBT1dMzyBrSv"}}]}